CHI 2026
Is It Still You? Attributing Authorship and Authenticity in AI-Assisted Romantic Communication
Early Abstract
In close relationships, how a difficult message is produced-visible effort, authorship -carries as much meaning as what it says. Generative AI can make text clearer and kinder, but it also risks weakening these costly signals. We examine how help level and a brief co-sign disclosure shape receivers' attributions of effort/ownership, perceived authenticity, and outcomes in two scenarios: apology and boundary requests. Study 1 instrumented real authoring to observe assistance behavior and curate a many-stimuli corpus. Study 2 used that corpus and tested an attributional pathway. Across scenarios, heavier drafting reduced perceived ownership and authenticity; competence/clarity improved but did not substitute for authenticity. In apologies, co-signing Tone increased authenticity and forgiveness, whereas co-signing Full decreased both; in boundary requests, co-signing yielded small or negative shifts. We contribute: (i) scenario-aware causal estimates for help disclosure; (ii) an empirically grounded, scenario-aware attributional account alongside a competence-integrity dissociation; and (iii) evidence of sender-receiver miscalibration.
Authors
Dr Guangrui Fan Taiyuan University of Science and Technology, Taiyuan, Shanxi Province, China Universiti Malaya, Kuala Lumpur, Malaysia
Ms Dandan Liu Faculty of Arts and Social Sciences, Universiti Malaya, Kuala Lumpur, Malaysia, s2134717@siswa.um.edu.my
Prof. Lihu Pan Computer Science and Technology, Taiyuan University of Science and Technology, Taiyuan, shanxi, China
Primary Subcommittee Selection
Interaction Beyond the Individual
Secondary Subcommittee Selection
Understanding People: Statistical & Quantitative Methods
Key Words
Computer Mediated Communication ; Emotion / Affective Computing ; Crowdsourced ; Empirical study that tells us about people ; Quantitative Methods
Submission Abstract
In intimate messaging, how a difficult note is produced signals effort and ownership. We study how AI assistance level (light tone rewrite vs heavy full draft) and a brief co-sign disclosure shape receiver attributions and outcomes in apology and boundary setting. Study 1 (N=152) instrumented authoring to build a many-stimuli corpus. Study 2 (N=704) tested effects in a mixed effects experiment. Heavier drafting reliably reduced perceived ownership and authenticity; clarity/competence gains did not compensate. In apologies, co-signing a tone rewrite increased authenticity and forgiveness; co-signing a full draft decreased both. In boundary requests, co-signing yielded small or negative shifts. Stimulus-level analyses tied idiosyncratic "voice" cues to ownership/authenticity, and revealed sender-receiver miscalibration. We contribute: (i) scenario-aware causal estimates for help and disclosure; (ii) an empirically grounded, scenario-aware attributional account alongside a competence–integrity dissociation; (iii) evidence of sender–receiver miscalibration; and (iv) design guidance voice preserving defaults, ownership restoring scaffolds, and CPM disclosure.


CHI 2026

When Help Hurts: Verification Load and Fatigue with AI Coding Assistants
Early Abstract
We present a mixed‑methods study of how three AI interaction modes for programming—Inline suggestions, Chat prompts, and Structured forms—shape developers’ workload, performance, stress, and fatigue across realistic tasks. A within‑subjects AI cohort completed all three modes with counterbalanced task–mode pairings, and a control cohort completed the same tasks without AI. We combine unit‑test correctness, completion time, and subjective measures with IDE logs to construct a “verification‑load” index that captures failures, time to first compile, churn, pauses, and context switches. AI assistance generally reduced workload and time and improved correctness, but effects depended on task complexity and expertise: Inline favored simple work, Chat excelled on complex logic, and Structured aided novices on mid‑complexity specifications. Fatigue and stress rose with repeated use; increases were partly explained by verification‑load. Qualitative interviews converged on themes of speed versus certainty, trust shocks, scaffolding needs, and review fatigue. We discuss design implications for adaptive, transparency‑aware tools.
Authors
Dr Guangrui Fan Taiyuan University of Science and Technology, Taiyuan, Shanxi Province, China Universiti Malaya, Kuala Lumpur, Malaysia
Ms Dandan Liu Faculty of Arts and Social Sciences, Universiti Malaya, Kuala Lumpur, Malaysia, s2134717@siswa.um.edu.my
Prof. Lihu Pan Computer Science and Technology, Taiyuan University of Science and Technology, Taiyuan, shanxi, China
Pro Rui Zhang Taiyuan University of Science and Technology, School of Computer Science and Technology, Taiyuan, ShanXi, China
Primary Subcommittee Selection
Understanding People: Mixed & Alternative Methods
Secondary Subcommittee Selection
User Experience & Usability
Key Words
Programming/Development Support ; Workplaces ; Empirical study that tells us about how people use a system ; Interview ; Survey


CHI 2026
Submission
contact : Guangrui Fan (fgr@tyust.edu.cn)

Co‑Adaptive Eco‑Nudging: A Privacy‑Preserving Contextual Bandit with User‑Taught Preferences in Everyday Browsing
Early Abstract
Digital eco‑nudges often help in the moment yet risk annoyance, privacy concerns, and short‑lived impact. We present a privacy‑preserving, on‑device, co‑adaptive nudging system that (i) begins with user‑initiated goals and preferred tone, (ii) uses a constrained contextual bandit to decide whether/when/what to nudge under user‑set quiet hours and daily prompt budgets, and (iii) pairs each prompt with a concise “Why this?” explanation and one‑tap feedback that directly trains the policy. Across two field studies with matched prompt budgets and ablation‑controlled message content, context‑tailored prompts outperformed generic alternatives, and the co‑adaptive policy further improved outcomes by learning restraint (including choosing to do nothing). Effects partially persisted during withdrawal, and net energy savings remained positive after accounting for system overhead. We formalize an Ethical–Efficacy Frontier relating autonomy to compliance and derive design guidance for responsible personalization: co‑adaptive control, transparent explanations, user‑governed constraints, and on‑device learning that minimizes data collection and carbon cost.
Authors
Dr Guangrui Fan Taiyuan University of Science and Technology, Taiyuan, Shanxi Province, China Universiti Malaya, Kuala Lumpur, Malaysia
Ms Dandan Liu Faculty of Arts and Social Sciences, Universiti Malaya, Kuala Lumpur, Malaysia, s2134717@siswa.um.edu.my
Prof. Lihu Pan Computer Science and Technology, Taiyuan University of Science and Technology, Taiyuan, shanxi, China
Primary Subcommittee Selection
Critical Computing, Sustainability, & Social Justice
Secondary Subcommittee Selection
Understanding People: Mixed & Alternative Methods
Key Words
Behavior Change ; Personalization ; Sustainability ; Tasks/Interruptions/Notification ; Field Study ; Prototyping/Implementation ; Survey
Submission Abstract
Digital eco‑nudges are widely deployed, yet their long‑term efficacy, ethical acceptability, and net environmental impact remain unclear. We report two field studies targeting routine online behaviors under strict parity of message content and delivery budgets. Study 1 shows that minimal, factual tailoring improves compliance over generic prompts when opportunities are defined independently of delivery. Study 2 introduces a privacy‑preserving, on‑device contextual bandit that learns when to act and when to DoNothing, achieving higher compliance at comparable prompt intensity while maintaining autonomy. We further operationalize an Ethical–Efficacy Frontier (EEF) to visualize autonomy–effectiveness trade‑offs, and compute an Energy Return on Investment (ROI) that nets behavior‑driven savings against measured system overhead. Withdrawal and follow‑up suggest partial persistence for high‑frequency behaviors. We contribute design and reporting practices—ablation parity, opportunity denominators, EEF, and net‑impact accounting—that make digital sustainability interventions more rigorous, transparent, and respectful, advancing sustainable HCI beyond “small changes.”


and 
WWW 2026
Dandan Liu, Aznul Qalid Md Sabri, Lihu Pan and Guangrui Fan	Audit‑of‑Audits for the Web: Bayesian Meta‑Evaluation that Yields Interval‑Valued, Threshold‑Aligned Fairness Claims	Web4Good		www2026-tracks_paper_137.pdf
209	Dandan Liu, Aznul Qalid Md Sabri, Lihu Pan and Guangrui Fan	From Solo Post to Shared Space: Evidence that Public LLM Replies Increase Cross‑Group Dialogue	Web4Good	

Guangrui Fan, Dandan Liu, Rui Zhang and Lihu Pan	Stable Long‑Run Exposure Fairness under Creator–Platform Co‑adaptation

Guangrui Fan, Dandan Liu, Lihu Pan, Rui Zhang and Aznul Qalid Md Sabri	Consent Boundaries by Play: CI-Grounded Micro-Games to Elicit Stable, UI-Robust Web Data-Sharing Preferences

